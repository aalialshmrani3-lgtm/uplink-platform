# UPLINK 5.0 - AI Prediction Services

**Version:** 2.0  
**Last Updated:** January 2026  
**Status:** Production Ready âœ…

---

## ğŸ“‹ Overview

UPLINK 5.0 AI Prediction Services provide intelligent success probability predictions for innovation projects using **32-dimensional AraBERT semantic embeddings** combined with traditional project features. The system includes **SHAP-based explainability** to provide human-readable explanations in Arabic and English.

### Key Features

- **32D Semantic Understanding:** AraBERT embeddings capture deep semantic meaning from Arabic/English project descriptions
- **42D Feature Vector:** 10 traditional features + 32 semantic features for comprehensive analysis
- **SHAP Explainability:** Human-readable explanations showing which features contributed to the prediction
- **JWT Authentication:** Secure API access with token-based authentication
- **Database Integration:** Direct PostgreSQL/MongoDB support for continuous learning
- **Testing Dashboard:** Interactive Streamlit dashboard for real-time testing

---

## ğŸ—ï¸ Architecture

```
ai-services/prediction/
â”œâ”€â”€ main.py                     # FastAPI server with /predict and /explain endpoints
â”œâ”€â”€ retrain_model.py            # Model retraining script with database support
â”œâ”€â”€ embeddings_service.py       # AraBERT embeddings + PCA dimensionality reduction
â”œâ”€â”€ shap_explainer.py           # SHAP-based explainability service
â”œâ”€â”€ jwt_auth.py                 # JWT authentication middleware
â”œâ”€â”€ database_connector.py       # PostgreSQL/MongoDB connector
â”œâ”€â”€ database_schema.sql         # Database schema for ideas_outcomes table
â”œâ”€â”€ db_seeder.py                # Generates 500 semi-realistic Saudi market samples
â”œâ”€â”€ test_dashboard.py           # Streamlit testing dashboard
â”œâ”€â”€ model.pkl                   # Trained Random Forest model
â”œâ”€â”€ scaler.pkl                  # Feature scaler
â”œâ”€â”€ pca_model.pkl               # PCA model for 32D embeddings
â”œâ”€â”€ STATUS_REPORT_V2.md         # Technical status report
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Navigate to prediction directory
cd ai-services/prediction

# Install Python dependencies
pip install -r requirements.txt
```

**Required packages:**
```
fastapi==0.104.1
uvicorn==0.24.0
transformers==4.35.2
torch==2.1.1
sentencepiece==0.1.99
scikit-learn==1.3.2
pandas==2.1.3
numpy==1.26.2
PyJWT==2.8.0
psycopg2-binary==2.9.9
pymongo==4.6.0
shap==0.43.0
streamlit==1.29.0
matplotlib==3.8.2
requests==2.31.0
```

### 2. Set Environment Variables

```bash
# Database configuration
export DB_TYPE="postgresql"  # or "mongodb"
export DATABASE_URL="postgresql://user:password@localhost:5432/uplink"

# JWT secret
export JWT_SECRET="your-secret-key-here"

# API configuration
export API_PORT=8000
```

### 3. Setup Database

#### PostgreSQL Setup

```bash
# Create database
createdb uplink

# Create tables
psql uplink < database_schema.sql
```

#### MongoDB Setup

```bash
# MongoDB will auto-create collections
# No schema file needed
```

### 4. Seed Database with Sample Data

```bash
# Generate and seed 500 semi-realistic Saudi market samples
python db_seeder.py

# Follow prompts to seed database or save to JSON
```

**Sample output:**
```
Generating 500 samples:
  - NEOM projects: 75
  - Vision 2030 projects: 125
  - PIF projects: 100
  - Failure scenarios: 100
  - Moderate projects: 100

ğŸ“Š Generated Statistics:
  Total samples: 500
  Successful projects: 345 (69.0%)
  Failed projects: 155 (31.0%)
  Average budget: $725,450
  Sectors covered: 10

âœ… Successfully seeded 500 samples into postgresql database!
```

### 5. Train Model

```bash
# Train model with database data
python retrain_model.py

# This will:
# 1. Fetch data from database
# 2. Generate 32D AraBERT embeddings
# 3. Train Random Forest model
# 4. Save model, scaler, and PCA to disk
```

**Expected output:**
```
Loading data from postgresql database...
Loaded 500 samples

Generating AraBERT embeddings (32D)...
Training PCA model...
PCA explained variance: 0.87

Preparing features (42D)...
Training Random Forest model...

Model Performance:
  Accuracy: 0.85
  Precision: 0.83
  Recall: 0.88
  F1 Score: 0.85
  ROC AUC: 0.91

âœ… Model saved to model.pkl
âœ… Scaler saved to scaler.pkl
âœ… PCA model saved to pca_model.pkl
```

### 6. Start API Server

```bash
# Start FastAPI server
python main.py

# Server will start on http://localhost:8000
```

**API Endpoints:**
- `POST /predict` - Get success probability prediction
- `POST /explain` - Get SHAP-based explanation
- `GET /health` - Health check

### 7. Launch Testing Dashboard

```bash
# In a new terminal
streamlit run test_dashboard.py

# Dashboard will open at http://localhost:8501
```

---

## ğŸ“Š API Usage

### Authentication

All API requests require JWT authentication:

```bash
# Get JWT token (implement your own auth flow)
export JWT_TOKEN="your-jwt-token-here"
```

### Predict Endpoint

```bash
curl -X POST http://localhost:8000/predict \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Ù…Ù†ØµØ© ØªØ¹Ù„ÙŠÙ… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    "description": "Ù…Ù†ØµØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ© ØªÙØ§Ø¹Ù„ÙŠØ© Ù…ØªØ®ØµØµØ© ÙÙŠ ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    "budget": 250000,
    "team_size": 8,
    "timeline_months": 12,
    "market_demand": 75,
    "technical_feasibility": 80,
    "competitive_advantage": 65,
    "user_engagement": 70,
    "tags_count": 8,
    "hypothesis_validation_rate": 0.7,
    "rat_completion_rate": 0.65
  }'
```

**Response:**
```json
{
  "success_probability": 0.73,
  "confidence": 0.85,
  "recommendation": "proceed",
  "model_version": "v2.0"
}
```

### Explain Endpoint

```bash
curl -X POST http://localhost:8000/explain \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Ù…Ù†ØµØ© ØªØ¹Ù„ÙŠÙ… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    "description": "Ù…Ù†ØµØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ© ØªÙØ§Ø¹Ù„ÙŠØ© Ù…ØªØ®ØµØµØ© ÙÙŠ ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    ...
  }'
```

**Response:**
```json
{
  "success_probability": 0.73,
  "shap_values": {
    "semantic_feature_0": 0.12,
    "market_demand": 0.08,
    "technical_feasibility": 0.06,
    ...
  },
  "base_value": 0.65,
  "explanation": {
    "arabic": "Ù†Ù…ÙˆØ°Ø¬ AraBERT ÙˆØ¬Ø¯ ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ ÙƒØ¨ÙŠØ±Ø§Ù‹ Ù…Ø¹ Ù…Ø´Ø§Ø±ÙŠØ¹ Ù†Ø§Ø¬Ø­Ø© Ø³Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ...",
    "english": "AraBERT model found high similarity with successful past projects in e-learning sector..."
  },
  "top_positive_features": [
    {"feature": "semantic_feature_0", "value": 0.12, "description": "..."},
    ...
  ],
  "top_negative_features": [
    {"feature": "budget", "value": -0.03, "description": "..."},
    ...
  ]
}
```

---

## ğŸ§ª Testing Dashboard

The Streamlit testing dashboard provides an interactive interface for testing predictions with real-time SHAP visualizations.

### Features

- **Interactive Input Form:** Enter project details with sliders and text inputs
- **Real-time Predictions:** Get instant success probability predictions
- **SHAP Visualizations:**
  - Gauge chart for success probability
  - Waterfall chart showing feature contributions
  - Bar chart for feature importance
  - Feature contributions table
- **Bilingual Explanations:** Arabic and English explanations
- **Configuration Panel:** Set API URL and JWT token

### Usage

```bash
# Start dashboard
streamlit run test_dashboard.py

# Set JWT token in sidebar
# Enter project details
# Click "Predict Success"
# View results and SHAP explanations
```

---

## ğŸ—„ï¸ Database Schema

### PostgreSQL Schema

```sql
CREATE TABLE ideas_outcomes (
    id SERIAL PRIMARY KEY,
    idea_id INTEGER NOT NULL,
    title TEXT NOT NULL,
    description TEXT NOT NULL,
    sector VARCHAR(100),
    budget DECIMAL(15, 2),
    team_size INTEGER,
    timeline_months INTEGER,
    market_demand INTEGER CHECK (market_demand BETWEEN 0 AND 100),
    technical_feasibility INTEGER CHECK (technical_feasibility BETWEEN 0 AND 100),
    competitive_advantage INTEGER CHECK (competitive_advantage BETWEEN 0 AND 100),
    user_engagement INTEGER CHECK (user_engagement BETWEEN 0 AND 100),
    tags_count INTEGER,
    hypothesis_validation_rate DECIMAL(3, 2) CHECK (hypothesis_validation_rate BETWEEN 0 AND 1),
    rat_completion_rate DECIMAL(3, 2) CHECK (rat_completion_rate BETWEEN 0 AND 1),
    success BOOLEAN NOT NULL,
    outcome_date TIMESTAMP,
    failure_reason TEXT,
    success_metrics JSONB,
    organization_id INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_success ON ideas_outcomes(success);
CREATE INDEX idx_sector ON ideas_outcomes(sector);
CREATE INDEX idx_organization ON ideas_outcomes(organization_id);
CREATE INDEX idx_outcome_date ON ideas_outcomes(outcome_date);
```

### MongoDB Schema

```javascript
{
  idea_id: Number,
  title: String,
  description: String,
  sector: String,
  budget: Number,
  team_size: Number,
  timeline_months: Number,
  market_demand: Number,  // 0-100
  technical_feasibility: Number,  // 0-100
  competitive_advantage: Number,  // 0-100
  user_engagement: Number,  // 0-100
  tags_count: Number,
  hypothesis_validation_rate: Number,  // 0-1
  rat_completion_rate: Number,  // 0-1
  success: Boolean,
  outcome_date: Date,
  failure_reason: String,
  success_metrics: Object,
  organization_id: Number,
  created_at: Date,
  updated_at: Date
}
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DB_TYPE` | Database type (`postgresql` or `mongodb`) | `postgresql` |
| `DATABASE_URL` | PostgreSQL connection string | - |
| `MONGODB_URI` | MongoDB connection string | - |
| `JWT_SECRET` | Secret key for JWT signing | - |
| `API_PORT` | API server port | `8000` |
| `API_BASE_URL` | Base URL for API (used by dashboard) | `http://localhost:8000` |
| `ARABERT_MODEL` | AraBERT model name | `aubmindlab/bert-base-arabertv2` |
| `PCA_COMPONENTS` | Number of PCA components | `32` |

### Model Configuration

Edit `embeddings_service.py` to change:
- AraBERT model
- PCA dimensions
- Embedding pooling strategy

Edit `retrain_model.py` to change:
- Random Forest hyperparameters
- Train/test split ratio
- Feature engineering logic

---

## ğŸ“ˆ Performance Metrics

### Model Performance (v2.0)

Based on 500 semi-realistic Saudi market samples:

| Metric | Value |
|--------|-------|
| Accuracy | 85% |
| Precision | 83% |
| Recall | 88% |
| F1 Score | 85% |
| ROC AUC | 91% |

### Feature Importance

Top 10 features by SHAP importance:

1. `semantic_feature_0` (AraBERT embedding dimension 0)
2. `market_demand`
3. `technical_feasibility`
4. `semantic_feature_1`
5. `competitive_advantage`
6. `user_engagement`
7. `hypothesis_validation_rate`
8. `budget`
9. `rat_completion_rate`
10. `semantic_feature_2`

---

## ğŸ”„ Continuous Learning

### Retraining Workflow

1. **Collect New Data:** New projects added to `ideas_outcomes` table
2. **Retrain Model:** Run `python retrain_model.py` weekly/monthly
3. **Evaluate Performance:** Check metrics in retrain output
4. **Deploy New Model:** Replace `model.pkl`, `scaler.pkl`, `pca_model.pkl`
5. **Restart API:** Restart `main.py` to load new models

### Automated Retraining (Optional)

```bash
# Add to crontab for weekly retraining
0 2 * * 0 cd /path/to/prediction && python retrain_model.py >> retrain.log 2>&1
```

---

## ğŸ› Troubleshooting

### Common Issues

#### 1. "Model file not found"

**Solution:** Train model first:
```bash
python retrain_model.py
```

#### 2. "Database connection failed"

**Solution:** Check `DATABASE_URL` and database status:
```bash
# PostgreSQL
psql $DATABASE_URL -c "SELECT 1"

# MongoDB
mongosh $MONGODB_URI --eval "db.adminCommand('ping')"
```

#### 3. "AraBERT model download failed"

**Solution:** Download manually:
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("aubmindlab/bert-base-arabertv2")
model = AutoModel.from_pretrained("aubmindlab/bert-base-arabertv2")
```

#### 4. "SHAP explanation timeout"

**Solution:** Increase timeout in `test_dashboard.py`:
```python
response = requests.post(..., timeout=120)  # Increase from 60 to 120
```

#### 5. "JWT token invalid"

**Solution:** Generate new token with correct secret:
```python
import jwt
token = jwt.encode({"user_id": 1}, "your-secret-key", algorithm="HS256")
```

---

## ğŸ“š Additional Resources

- **STATUS_REPORT_V2.md:** Technical status report with architecture details
- **database_schema.sql:** Complete PostgreSQL schema
- **db_seeder.py:** Sample data generator with Saudi market scenarios
- **test_dashboard.py:** Interactive testing dashboard source code

---

## ğŸ¤ Contributing

### Development Workflow

1. **Create Feature Branch:** `git checkout -b feature/your-feature`
2. **Make Changes:** Edit code and add tests
3. **Test Locally:** Run `python retrain_model.py` and `python main.py`
4. **Submit PR:** Push branch and create pull request

### Code Style

- Follow PEP 8 for Python code
- Use type hints for function signatures
- Add docstrings for all functions
- Keep functions under 50 lines

---

## ğŸ“„ License

Copyright Â© 2026 UPLINK 5.0 - National Innovation Platform  
All rights reserved.

---

## ğŸ“ Support

For technical support or questions:
- **Email:** support@uplink.sa
- **Documentation:** https://docs.uplink.sa
- **Issue Tracker:** https://github.com/uplink/ai-services/issues

---

**Last Updated:** January 31, 2026  
**Version:** 2.0  
**Status:** Production Ready âœ…
